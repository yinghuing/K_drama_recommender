{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinghuing/K_drama_recommender/blob/master/chapter5/implement_agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 0: ハンズオンの準備\n",
        "---"
      ],
      "metadata": {
        "id": "2SXS3qjHO5J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要なライブラリのインストール"
      ],
      "metadata": {
        "id": "hjpQpmcrkcA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain langchain-openai langchain-community langchainhub langgraph tavily-python chromadb tiktoken"
      ],
      "metadata": {
        "id": "xNJhQoCptr7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.3.7 langchain-community==0.3.7 langchain-core==0.3.18 langchain-openai==0.2.9 langchain-text-splitters==0.3.2 langgraph==0.2.52 langgraph-checkpoint==2.0.5 langchainhub==0.1.21 tavily-python chromadb tiktoken"
      ],
      "metadata": {
        "id": "5L-ErUJuMtEx",
        "outputId": "2795c978-f373-4a19-f9a8-fd7eaa8b50a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.0/91.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m880.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep langchain"
      ],
      "metadata": {
        "id": "15QyqnKk1CTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep langgraph"
      ],
      "metadata": {
        "id": "z_GuZmGf1gKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API キーの設定\n",
        "*  左ナビゲーションで [**シークレット**] アイコン (鍵形のアイコン) をクリックします。\n",
        "*  [**新しいシークレットを追加**] をクリックし、`LANGCHAIN_API_KEY`、`OPENAI_API_KEY`、`TAVILY_API_KEY` の 3 つを設定し、[**ノートブックからのアクセス**] を有効にします\n",
        "  *  `OPENAI_API_KEY` の [**値**] には指定されたキーを入力します。\n",
        "  *  `LANGCHAIN_API_KEY` と `TAVILY_API_KEY` の [**値**] にはご自身で取得したキーを入力してください。\n",
        "*  入力が完了したら、下のセルを実行します。"
      ],
      "metadata": {
        "id": "7Xq-T4gzi4ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3-Ha_aLspoO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pclZg_IOh173"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: LangGraph の基本\n",
        "---\n",
        "https://langchain-ai.github.io/langgraph/  \n",
        "[Intro to LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/)"
      ],
      "metadata": {
        "id": "IORhWlqn8Fgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 基本のチャットボットを作成する"
      ],
      "metadata": {
        "id": "O8mtRsNxExM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### State と Graph\n",
        "State は `messages` (Message のリスト) として状態を保持する。  \n",
        "[StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#stategraph)"
      ],
      "metadata": {
        "id": "PGqsSKUgBqPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# State を定義\n",
        "class State(TypedDict):\n",
        "    # Messages have the type \"list\". The `add_messages` function\n",
        "    # in the annotation defines how this state key should be updated\n",
        "    # (in this case, it appends messages to the list, rather than overwriting them)\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Graph (StateGraph) のインスタンスを作成\n",
        "graph_builder = StateGraph(State)"
      ],
      "metadata": {
        "id": "wHbCdUr2sd_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Node"
      ],
      "metadata": {
        "id": "epkfSxhhSxRZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3sdyngepk-"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# Chat model を用意\n",
        "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "# Node を定義\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [model.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Node を Graph に追加\n",
        "## 第 1 引数は Node の名前\n",
        "## 第 2 引数は Node の関数/オブジェクト\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.set_entry_point(\"chatbot\")"
      ],
      "metadata": {
        "id": "NN7N7UKx4Vp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_builder.set_finish_point(\"chatbot\")"
      ],
      "metadata": {
        "id": "kItYTNZB4WdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph のコンパイル"
      ],
      "metadata": {
        "id": "f_HZ9x03DeI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "rflzG6iL4bbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph の可視化"
      ],
      "metadata": {
        "id": "uUjJ5tDqDw1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "2lRlpFd44ylg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph の実行"
      ],
      "metadata": {
        "id": "br8y2kniDzo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.invoke({\"messages\": [(\"user\", \"LangGraph とは何か教えてください。\")]})"
      ],
      "metadata": {
        "id": "cav-xTEvZ1Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event = graph.invoke({\"messages\": [(\"user\", \"LangGraph とは何か教えてください。\")]})\n",
        "print(event[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "AC9mG99NFmuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "\n",
        "    event = graph.invoke({\"messages\": (\"user\", user_input)})\n",
        "    print(\"Assistant:\", event[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "I74txK125DyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools\n",
        "https://python.langchain.com/docs/concepts/#tools  \n",
        "https://python.langchain.com/docs/how_to/#tools  \n",
        "https://python.langchain.com/docs/integrations/tools/"
      ],
      "metadata": {
        "id": "TpSBwtpuoGnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tavily Search\n",
        "*  Tool として Tavily Search を使用する。\n",
        "*  この Tool は、インターネット検索をする機能を提供する。\n",
        "\n",
        "[langchain_community.tools.tavily_search.tool.TavilySearchResults](https://python.langchain.com/api_reference/community/tools/langchain_community.tools.tavily_search.tool.TavilySearchResults.html)"
      ],
      "metadata": {
        "id": "lCLDIgFdQwtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# TavilySearchResults のインスタンスを作成\n",
        "tool = TavilySearchResults(max_results=10)\n",
        "\n",
        "# Tool のリストを作成\n",
        "tools = [tool]\n",
        "\n",
        "# Tool も Runnable であり invoke メソッドで実行できる\n",
        "tool.invoke(\"LangGraph におけるノードとは何ですか。\")"
      ],
      "metadata": {
        "id": "WedKS69VpIPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# State を定義\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Graph (StateGraph) のインスタンスを作成\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Chat model のインスタンスを作成\n",
        "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "# 使用できる Tool の情報を Chat model 渡す\n",
        "## 上で作成した Tool のリストをバインドする\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "# LLM を呼び出す Node を定義\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "# Node を Graph に追加\n",
        "graph_builder.add_node(\"chatbot\", chatbot)"
      ],
      "metadata": {
        "id": "VFhLPHBIoPpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool で処理を実行する Node を追加する\n",
        "ここでは、LangGraph に予め用意されている `ToolNode` を使用する。  \n",
        "[ToolNode](https://langchain-ai.github.io/langgraph/reference/prebuilt/#toolnode)"
      ],
      "metadata": {
        "id": "JfEBFzkK1Dpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "# ToolNode のインスタンスを作成\n",
        "## 上で作成した Tool のリストを渡す\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "\n",
        "# Node を Graph に追加\n",
        "graph_builder.add_node(\"tools\", tool_node)"
      ],
      "metadata": {
        "id": "RGO8NfWZzlL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この Node のクラスは自分で定義することもできる。"
      ],
      "metadata": {
        "id": "66-mEvDpLN3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# from langchain_core.messages import ToolMessage\n",
        "\n",
        "# class BasicToolNode:\n",
        "#     \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
        "\n",
        "#     def __init__(self, tools: list) -> None:\n",
        "#         self.tools_by_name = {tool.name: tool for tool in tools}\n",
        "\n",
        "#     def __call__(self, inputs: dict):\n",
        "#         if messages := inputs.get(\"messages\", []):\n",
        "#             message = messages[-1]\n",
        "#         else:\n",
        "#             raise ValueError(\"No message found in input\")\n",
        "#         outputs = []\n",
        "#         for tool_call in message.tool_calls:\n",
        "#             tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
        "#                 tool_call[\"args\"]\n",
        "#             )\n",
        "#             outputs.append(\n",
        "#                 ToolMessage(\n",
        "#                     content=json.dumps(tool_result),\n",
        "#                     name=tool_call[\"name\"],\n",
        "#                     tool_call_id=tool_call[\"id\"],\n",
        "#                 )\n",
        "#             )\n",
        "#         return {\"messages\": outputs}\n",
        "\n",
        "# tool_node = BasicToolNode(tools=[tool])\n",
        "# graph_builder.add_node(\"tools\", tool_node)"
      ],
      "metadata": {
        "id": "3wdyKkUNTaU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tool を使うかどうか条件分岐する Edge を追加する\n",
        "ここでは、LangGraph にあらかじめ用意されている `tools_conditon` を使用する。  \n",
        "[tools_condition](https://langchain-ai.github.io/langgraph/reference/prebuilt/?#tools_condition)"
      ],
      "metadata": {
        "id": "C4wVuswe2lad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import tools_condition\n",
        "\n",
        "# Edge を Graph に追加\n",
        "## Node \"chatbot\" の処理後に条件分岐を行う\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")"
      ],
      "metadata": {
        "id": "CJQgiy_X136L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この Edge のクラスは自分で定義することもできる。"
      ],
      "metadata": {
        "id": "NL4MaTrMlXWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from typing import Literal\n",
        "\n",
        "# def route_tools(\n",
        "#     state: State,\n",
        "# ) -> Literal[\"tools\", \"__end__\"]:\n",
        "#     \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n",
        "\n",
        "#     has tool calls. Otherwise, route to the end.\"\"\"\n",
        "#     if isinstance(state, list):\n",
        "#         ai_message = state[-1]\n",
        "#     elif messages := state.get(\"messages\", []):\n",
        "#         ai_message = messages[-1]\n",
        "#     else:\n",
        "#         raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
        "#     if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
        "#         return \"tools\"\n",
        "#     return \"__end__\"\n",
        "\n",
        "# # The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"__end__\" if\n",
        "# # it is fine directly responding. This conditional routing defines the main agent loop.\n",
        "# graph_builder.add_conditional_edges(\n",
        "#     \"chatbot\",\n",
        "#     route_tools,\n",
        "#     # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
        "#     # It defaults to the identity function, but if you\n",
        "#     # want to use a node named something else apart from \"tools\",\n",
        "#     # You can update the value of the dictionary to something else\n",
        "#     # e.g., \"tools\": \"my_tools\"\n",
        "#     {\"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
        "# )\n",
        "# # Any time a tool is called, we return to the chatbot to decide the next step\n",
        "# graph_builder.add_edge(\"tools\", \"chatbot\")"
      ],
      "metadata": {
        "id": "ooRp8X2bUSL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tools → chatbot への Edge を追加\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "\n",
        "# エントリーポイントをセット\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "\n",
        "# Graph をコンパイル\n",
        "graph = graph_builder.compile()"
      ],
      "metadata": {
        "id": "9HPu1O_bUlH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph を可視化"
      ],
      "metadata": {
        "id": "JLbUS7A4NF_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "yjxo_JyvUtLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph を実行"
      ],
      "metadata": {
        "id": "bTKLhx41NbLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph.invoke({\"messages\": [(\"user\", \"LangGraph におけるノードとは何ですか。\")]})"
      ],
      "metadata": {
        "id": "yDp9qZildHNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "event = graph.invoke({\"messages\": [(\"user\", \"LangGraph におけるノードとは何ですか。\")]})\n",
        "print(event[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "pmU6nSyBNWDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "インタラクティブに実行"
      ],
      "metadata": {
        "id": "A1nk_2jWPDz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
        "    print(\"Assistant:\", response[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "RgAOecx-lcNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### まとめたコード"
      ],
      "metadata": {
        "id": "KyHCdT3gPRAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "\n",
        "tool = TavilySearchResults(max_results=5)\n",
        "tools = [tool]\n",
        "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")\n",
        "graph = graph_builder.compile()\n",
        "\n",
        "user_input = \"LangGraph におけるノードとは何ですか。\"\n",
        "\n",
        "response = graph.invoke({\"messages\": [(\"user\", user_input)]})\n",
        "print(\"Answer:\", response[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "v4-6WZrL6V1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: チャットボットに会話履歴を追加する\n",
        "---"
      ],
      "metadata": {
        "id": "T7gNKZCQvy-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpointer\n",
        "* ここでは、インメモリに履歴データを保存する `MemorySaver` を使用する。\n",
        "* データベースにデータを保存する場合は、`SqliteSaver` や `PostgresSaver` などを利用できる。\n",
        "  \n",
        "[MemorySaver](https://langchain-ai.github.io/langgraph/reference/checkpoints/#memorysaver)"
      ],
      "metadata": {
        "id": "Z71eDDX2S2Sx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "memory = MemorySaver()"
      ],
      "metadata": {
        "id": "KTbLPjU5xp8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "from langchain_core.messages import BaseMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import StateGraph\n",
        "from langgraph.graph.message import add_messages\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# State を定義\n",
        "class State(TypedDict):\n",
        "    messages: Annotated[list, add_messages]\n",
        "\n",
        "# Graph のインスタンスを作成\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Tool の用意\n",
        "tool = TavilySearchResults(max_results=2)\n",
        "tools = [tool]\n",
        "\n",
        "# Chat model のインスタンスを作成\n",
        "model = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
        "\n",
        "# Chat model に Tool の情報を渡す\n",
        "llm_with_tools = model.bind_tools(tools)\n",
        "\n",
        "# Node と Edge の追加\n",
        "def chatbot(state: State):\n",
        "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
        "\n",
        "graph_builder.add_node(\"chatbot\", chatbot)\n",
        "\n",
        "tool_node = ToolNode(tools=[tool])\n",
        "graph_builder.add_node(\"tools\", tool_node)\n",
        "\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"chatbot\",\n",
        "    tools_condition,\n",
        ")\n",
        "# Any time a tool is called, we return to the chatbot to decide the next step\n",
        "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
        "graph_builder.set_entry_point(\"chatbot\")"
      ],
      "metadata": {
        "id": "vEX6DZY0xwZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checkpointer を指定して Graph をコンパイル"
      ],
      "metadata": {
        "id": "C6JUiemqUBZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = graph_builder.compile(checkpointer=memory)"
      ],
      "metadata": {
        "id": "tl7lU_wd9BCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph を可視化"
      ],
      "metadata": {
        "id": "bpIXVN6vUU8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "muqPUoBt8r7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph を実行"
      ],
      "metadata": {
        "id": "Llw_y07hUfov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"a678\"}}"
      ],
      "metadata": {
        "id": "GznIGkUF856-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"User: \")\n",
        "    if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "        print(\"Goodbye!\")\n",
        "        break\n",
        "    response = graph.invoke({\"messages\": [(\"user\", user_input)]}, config, stream_mode=\"values\")\n",
        "    print(\"Assistant:\", response[\"messages\"][-1].content)"
      ],
      "metadata": {
        "id": "irZeJr7Z-NdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 3: LangGraph Adaptive RAG\n",
        "---\n",
        "https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_adaptive_rag/"
      ],
      "metadata": {
        "id": "pQayogTccT-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Index と Retiever を作成する\n",
        "*  Documentloader には `WebBaseLoader` を使用\n",
        "*  Text Splitter には `RecursiveCharacterTextSplitter` を使用\n",
        "*  Embeddings model には `OpenAIEmbeddings` を使用\n",
        "*  Vector store には `Chroma` を使用"
      ],
      "metadata": {
        "id": "INWGcoHEcmM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Embedding model を用意\n",
        "embd = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "# RAG 用にロードする Web サイトの URL を指定\n",
        "urls = [\n",
        "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "]\n",
        "\n",
        "# ドキュメント (Web サイト) をロード\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# ドキュメントをチャンクに分割\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=500, chunk_overlap=100\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Vector store と Retriever を作成\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma\",\n",
        "    embedding=embd,\n",
        ")\n",
        "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={'k': 10})"
      ],
      "metadata": {
        "id": "5ooCHfG_4--6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM による各種の Chain を作成する"
      ],
      "metadata": {
        "id": "GAAMvtFDe6ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### question_router\n",
        "ユーザーの入力について、Vector store に問い合わせるか Web 検索するか決定する"
      ],
      "metadata": {
        "id": "14H3uuQFrLPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "# Data model\n",
        "## ユーザークエリを最適なデータソースにルーティングするためのモデルを定義している。\n",
        "## このモデルには、datasource という必須の属性があり、その値は \"vectorstore\" または \"web_search\" のいずれかでなければならない。\n",
        "class RouteQuery(BaseModel):\n",
        "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
        "\n",
        "    datasource: Literal[\"vectorstore\", \"web_search\"] = Field(\n",
        "        ...,\n",
        "        description=\"Given a user question choose to route it to web search or a vectorstore.\",\n",
        "    )\n",
        "\n",
        "\n",
        "# Chat model を用意\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "## Chat model に対して出力する形式を指定\n",
        "## https://api.python.langchain.com/en/latest/chat_models/langchain_community.chat_models.openai.ChatOpenAI.html#langchain_community.chat_models.openai.ChatOpenAI.with_structured_output\n",
        "structured_llm_router = model.with_structured_output(RouteQuery)\n",
        "\n",
        "# Prompt template を作成\n",
        "system = \"\"\"You are an expert at routing a user question to a vectorstore or web search.\n",
        "The vectorstore contains documents related to agents, prompt engineering, and adversarial attacks.\n",
        "Use the vectorstore for questions on these topics. Otherwise, use web-search.\"\"\"\n",
        "route_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"{question}\"),\n",
        "    ]\n",
        ")\n",
        "# Chain を定義\n",
        "## この Chain はユーザーのクエリに対して、`datasource` の値として `vectorstore` か `web_search` を返す\n",
        "question_router = route_prompt | structured_llm_router"
      ],
      "metadata": {
        "id": "1eYO464OeR_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動作を確認"
      ],
      "metadata": {
        "id": "uo4mlI-dOjGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    question_router.invoke(\n",
        "        {\"question\": \"明日の東京の天気予報は何ですか。\"}\n",
        "    )\n",
        ")\n",
        "print(question_router.invoke({\"question\": \"Agent の Memory とは何ですか。\"}))"
      ],
      "metadata": {
        "id": "0OCMmA4FOboi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### retrieval_grader\n",
        "Retriever が取得したドキュメントがユーザーの入力に対して関連のある適切なものか判定して \"yes\" か \"no\" を返す"
      ],
      "metadata": {
        "id": "Hc3zHFMhryJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class GradeDocuments(BaseModel):\n",
        "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Chat model を用意\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Prompt template を作成\n",
        "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
        "    If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
        "    It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
        "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
        "grade_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain を定義\n",
        "retrieval_grader = grade_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "Njat1uCHivpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動作を確認"
      ],
      "metadata": {
        "id": "R1YemhosO-yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"agent memory\"\n",
        "docs = retriever.invoke(question)\n",
        "doc_txt = docs[1].page_content\n",
        "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))\n",
        "len(docs)"
      ],
      "metadata": {
        "id": "cUmnRbYPO-OP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### rag_chain\n",
        "Retriever または Web 検索 Tool によって取得したドキュメントを使用して、ユーザーの入力に対する回答を生成する"
      ],
      "metadata": {
        "id": "aPeqOP4gt2kx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Prompt を用意\n",
        "## https://smith.langchain.com/hub/rlm/rag-prompt?organizationId=5416e3b5-a584-5319-ad04-ae41aaac8e2b\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "# Chat model を用意\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "\n",
        "# Post-processing\n",
        "# def format_docs(docs):\n",
        "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "# Chain\n",
        "rag_chain = prompt | model | StrOutputParser()\n"
      ],
      "metadata": {
        "id": "hDuoXsQzt5kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動作を確認"
      ],
      "metadata": {
        "id": "61Piqb5TPW1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
        "print(generation)"
      ],
      "metadata": {
        "id": "oqMf9DrLPaWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### hallucination_grader\n",
        "`rag_chain` による回答がグラウンディングされている (ハルシネーションでない) ことを判定して `yes` か `no` を返す"
      ],
      "metadata": {
        "id": "A7UNzsUo0Sfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class GradeHallucinations(BaseModel):\n",
        "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Chat model を用意\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm_grader = model.with_structured_output(GradeHallucinations)\n",
        "\n",
        "# Prompt template を作成\n",
        "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\n",
        "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
        "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain を定義\n",
        "hallucination_grader = hallucination_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "ztS3TpaLv7O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動作を確認"
      ],
      "metadata": {
        "id": "d6sr3aU_RLgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
      ],
      "metadata": {
        "id": "T-DtDxU4RKOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### answer_grader\n",
        "`rag_chain` による回答がユーザーに入力に対して適正である (関連している/解決している) ことを判定して `yes` か `no` を返す"
      ],
      "metadata": {
        "id": "hVqI3TRM1toD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data model\n",
        "class GradeAnswer(BaseModel):\n",
        "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
        "\n",
        "    binary_score: str = Field(\n",
        "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
        "    )\n",
        "\n",
        "\n",
        "# Chat model を用意\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "structured_llm_grader = model.with_structured_output(GradeAnswer)\n",
        "\n",
        "# Prompt template の作成\n",
        "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n\n",
        "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\"\"\"\n",
        "answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain を定義\n",
        "answer_grader = answer_prompt | structured_llm_grader"
      ],
      "metadata": {
        "id": "5mDfZIdI1YGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動作を確認"
      ],
      "metadata": {
        "id": "RjJ40c8kRyAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_grader.invoke({\"question\": question, \"generation\": generation})"
      ],
      "metadata": {
        "id": "3iwwmnWaRw_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### question_rewriter\n",
        "`retrieval_grader` または `answer_grader` で `no` と判定された場合、適切な回答を得やすいようにユーザーの入力を書き直す"
      ],
      "metadata": {
        "id": "0cEw4mn85Gg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat model を用意\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
        "\n",
        "# Prompt template を作成\n",
        "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
        "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
        "re_write_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Chain を定義\n",
        "question_rewriter = re_write_prompt | model | StrOutputParser()"
      ],
      "metadata": {
        "id": "T_TsDw-w5y_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "動作を確認"
      ],
      "metadata": {
        "id": "vu19mLXESQk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_rewriter.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "1c9yYotpSPaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web 検索のための Tool を用意する"
      ],
      "metadata": {
        "id": "gyDcpqJq6n5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "web_search_tool = TavilySearchResults(max_results=5)"
      ],
      "metadata": {
        "id": "TrEwDqL-6A3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State を定義する"
      ],
      "metadata": {
        "id": "ygP8OZlo8MPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        question: question\n",
        "        generation: LLM generation\n",
        "        documents: list of documents\n",
        "    \"\"\"\n",
        "    question: str\n",
        "    generation: str\n",
        "    documents: List[str]"
      ],
      "metadata": {
        "id": "F5AkBABC7A29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph のコンポーネントを定義する\n",
        "ここまでに作った、Retriever、各種の Chain、Tool を使って、グラフにノードやエッジとして追加するための関数を定義していく"
      ],
      "metadata": {
        "id": "Vxkj3ACT8rhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "\n",
        "### Node ###\n",
        "\n",
        "# Retriever を実行する関数\n",
        "def retrieve(state):\n",
        "    \"\"\"\n",
        "    Retrieve documents\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, documents, that contains retrieved documents\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVE---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Retrieval\n",
        "    documents = retriever.invoke(question)\n",
        "    return {\"documents\": documents, \"question\": question}\n",
        "\n",
        "\n",
        "# 最終的に回答の生成を実行する関数\n",
        "def generate(state):\n",
        "    \"\"\"\n",
        "    Generate answer\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation, that contains LLM generation\n",
        "    \"\"\"\n",
        "    print(\"---GENERATE---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # RAG generation\n",
        "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
        "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n",
        "\n",
        "\n",
        "# Retriever によって取得されたドキュメントを評価する関数\n",
        "def grade_documents(state):\n",
        "    \"\"\"\n",
        "    Determines whether the retrieved documents are relevant to the question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with only filtered relevant documents\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Score each doc\n",
        "    filtered_docs = []\n",
        "    for d in documents:\n",
        "        score = retrieval_grader.invoke(\n",
        "            {\"question\": question, \"document\": d.page_content}\n",
        "        )\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            # retriever が取得したドキュメントがユーザーの入力に関連性があれば、filtered_docs に追加する\n",
        "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
        "            filtered_docs.append(d)\n",
        "        else:\n",
        "            # ドキュメントに関連性がなければ捨てる\n",
        "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
        "            continue\n",
        "    return {\"documents\": filtered_docs, \"question\": question}\n",
        "\n",
        "\n",
        "# ユーザーの質問を書き直す関数\n",
        "def transform_query(state):\n",
        "    \"\"\"\n",
        "    Transform the query to produce a better question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates question key with a re-phrased question\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---TRANSFORM QUERY---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "\n",
        "    # Re-write question\n",
        "    better_question = question_rewriter.invoke({\"question\": question})\n",
        "    return {\"documents\": documents, \"question\": better_question}\n",
        "\n",
        "\n",
        "# Web 検索を実行する関数\n",
        "def web_search(state):\n",
        "    \"\"\"\n",
        "    Web search based on the re-phrased question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): Updates documents key with appended web results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---WEB SEARCH---\")\n",
        "    question = state[\"question\"]\n",
        "\n",
        "    # Web search\n",
        "    docs = web_search_tool.invoke({\"query\": question})\n",
        "    web_results = \"\\n\".join([d[\"content\"] for d in docs])\n",
        "    web_results = Document(page_content=web_results)\n",
        "\n",
        "    return {\"documents\": web_results, \"question\": question}\n",
        "\n",
        "\n",
        "### Edge ###\n",
        "\n",
        "# Vectore store の検索か Web 検索かを判定する関数\n",
        "def route_question(state):\n",
        "    \"\"\"\n",
        "    Route question to web search or RAG.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ROUTE QUESTION---\")\n",
        "    question = state[\"question\"]\n",
        "    source = question_router.invoke({\"question\": question})\n",
        "\n",
        "    # question_router による判定の結果を返す\n",
        "    if source.datasource == \"web_search\":\n",
        "        print(\"---ROUTE QUESTION TO WEB SEARCH---\")\n",
        "        return \"web_search\"\n",
        "    elif source.datasource == \"vectorstore\":\n",
        "        print(\"---ROUTE QUESTION TO RAG---\")\n",
        "        return \"vectorstore\"\n",
        "\n",
        "\n",
        "# 最終的な回答の生成を行うか判定する関数\n",
        "def decide_to_generate(state):\n",
        "    \"\"\"\n",
        "    Determines whether to generate an answer, or re-generate a question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Binary decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
        "    state[\"question\"]\n",
        "    filtered_documents = state[\"documents\"]\n",
        "\n",
        "\n",
        "    if not filtered_documents:\n",
        "        # grade_documents ですべてのドキュメントが関連性なしと判定された場合\n",
        "        print(\n",
        "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
        "        )\n",
        "        return \"transform_query\"\n",
        "    else:\n",
        "        # grade_documents で関連性があると判定されたドキュメントがある場合\n",
        "        print(\"---DECISION: GENERATE---\")\n",
        "        return \"generate\"\n",
        "\n",
        "\n",
        "# 生成された回答が適切かを判定する関数\n",
        "def grade_generation_v_documents_and_question(state):\n",
        "    \"\"\"\n",
        "    Determines whether the generation is grounded in the document and answers question.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Decision for next node to call\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECK HALLUCINATIONS---\")\n",
        "    question = state[\"question\"]\n",
        "    documents = state[\"documents\"]\n",
        "    generation = state[\"generation\"]\n",
        "\n",
        "    # Generate の回答がハルシネーションでないことを判定する\n",
        "    score = hallucination_grader.invoke(\n",
        "        {\"documents\": documents, \"generation\": generation}\n",
        "    )\n",
        "    grade = score.binary_score\n",
        "\n",
        "    # 回答がハルシネーションでなかった場合、回答が適正であることを判定する\n",
        "    if grade == \"yes\":\n",
        "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
        "        # Check question-answering\n",
        "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
        "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
        "        grade = score.binary_score\n",
        "        if grade == \"yes\":\n",
        "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
        "            # どちらの判定も \"yes\" だった場合\n",
        "            return \"useful\"\n",
        "        else:\n",
        "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
        "            # 回答が適正でなかった場合\n",
        "            return \"not useful\"\n",
        "    else:\n",
        "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
        "        # 回答がハルシネーションと判定され場合\n",
        "        return \"not supported\""
      ],
      "metadata": {
        "id": "T_rfnirE8ZLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph を構成する"
      ],
      "metadata": {
        "id": "tDy59iM6GhS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph\n",
        "from langgraph.constants import END\n",
        "\n",
        "# Graph のインスタンスを作成\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Graph に Node を追加\n",
        "workflow.add_node(\"web_search\", web_search)  # web search\n",
        "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
        "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
        "workflow.add_node(\"generate\", generate)  # generatae\n",
        "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
        "\n",
        "# Graph に Edge を追加\n",
        "workflow.set_conditional_entry_point(\n",
        "    route_question,\n",
        "    # route_question の出力に応じて次にどのノードに進むか分岐\n",
        "    {\n",
        "        \"web_search\": \"web_search\",\n",
        "        \"vectorstore\": \"retrieve\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"web_search\", \"generate\")\n",
        "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"grade_documents\",\n",
        "    decide_to_generate,\n",
        "    # decide_to_generate の出力に応じて次にどのノードに進むか分岐\n",
        "    {\n",
        "        \"transform_query\": \"transform_query\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"generate\",\n",
        "    grade_generation_v_documents_and_question,\n",
        "    # grade_generation_v_documents_and_question の出力に応じて次にどのノードに進むか分岐\n",
        "    {\n",
        "        \"not supported\": \"generate\",\n",
        "        \"useful\": END,\n",
        "        \"not useful\": \"transform_query\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# Graph をコンパイル\n",
        "app = workflow.compile()"
      ],
      "metadata": {
        "id": "QpWLF49jBOCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graph を可視化"
      ],
      "metadata": {
        "id": "BEh_CsnCfMNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(app.get_graph().draw_mermaid_png()))\n",
        "except:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "TlUgEUMdHpYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## グラフを実行する"
      ],
      "metadata": {
        "id": "MLP32bFjGsmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# User Question\n",
        "# inputs = {\n",
        "#     \"question\": \"プロンプトエンジニアリングの手法を3つ挙げてください。\"\n",
        "# }\n",
        "\n",
        "inputs = {\n",
        "    \"question\": \"ももいろクローバーZのメンバーの名前を教えてください。\"\n",
        "}\n",
        "\n",
        "value = {}\n",
        "for output in app.stream(inputs):\n",
        "    for key, value in output.items():\n",
        "        # Node\n",
        "        print(f\"Node '{key}': Done\")\n",
        "    print(\"---\")\n",
        "\n",
        "# Final generation\n",
        "print(value[\"generation\"])"
      ],
      "metadata": {
        "id": "BGum6EGCGdqR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}