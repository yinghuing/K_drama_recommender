{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yinghuing/K_drama_recommender/blob/master/chapter4/exercise2/exercise2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 演習の準備\n",
        "---"
      ],
      "metadata": {
        "id": "2SXS3qjHO5J9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要なライブラリのインストール"
      ],
      "metadata": {
        "id": "6NvuYwKzaYt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q langchain langchain-openai langchain-community langchain-core langchain-pinecone pypdf docx2txt wikipedia pinecone"
      ],
      "metadata": {
        "id": "xNJhQoCptr7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain==0.3.7 langchain-openai==0.2.9 langchain-community==0.3.7 langchain-core==0.3.18 langchain-text-splitters==0.3.2 pypdf docx2txt wikipedia"
      ],
      "metadata": {
        "id": "5L-ErUJuMtEx",
        "outputId": "435a7a54-25a8-4f4b-a92b-e18fcedf18dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep langchain"
      ],
      "metadata": {
        "id": "15QyqnKk1CTN",
        "outputId": "65494def-fb72-47b8-96b3-a71d11b5f9b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain==0.3.7\n",
            "langchain-community==0.3.7\n",
            "langchain-core==0.3.18\n",
            "langchain-openai==0.2.9\n",
            "langchain-text-splitters==0.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain-pinecone==0.2.0 pinecone==5.3.1"
      ],
      "metadata": {
        "id": "6iXAnCxiuPh_",
        "outputId": "2d34f33f-0510-4514-dc12-178a14a161c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/419.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m409.6/419.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze | grep pinecone"
      ],
      "metadata": {
        "id": "Rvq-yrm6tivB",
        "outputId": "ec827213-bf7e-443f-b020-e7733e550ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "langchain-pinecone==0.2.0\n",
            "pinecone==5.3.1\n",
            "pinecone-client==5.0.1\n",
            "pinecone-plugin-inference==1.1.0\n",
            "pinecone-plugin-interface==0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API キーの設定\n",
        "\n",
        "*  左ナビゲーションで [**シークレット**] アイコン (鍵形のアイコン) をクリックします。\n",
        "*  [**新しいシークレットを追加**] をクリックし、`LANGCHAIN_API_KEY`、`OPENAI_API_KEY`、`PINECONE_API_KEY` の 3 つを設定し、[**ノートブックからのアクセス**] を有効にします。\n",
        "  *  `OPENAI_API_KEY` の [**値**] には指定されたキーを入力します。\n",
        "  *  `LANGCHAIN_API_KEY` と `PINECONE_API_KEY` の [**値**] にはご自身で取得したキーを入力してください。\n",
        "*  入力が完了したら、下のセルを実行します。"
      ],
      "metadata": {
        "id": "7Xq-T4gzi4ga"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r3-Ha_aLspoO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"default\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"PINECONE_API_KEY\"] = userdata.get('PINECONE_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## サンプルファイルのアップロード\n",
        "\n",
        "*  左ナビゲーションで [**ファイル**] アイコンをクリックします。\n",
        "*  開いた [ファイル] 欄の何もない部分で右クリックし、[**新しいフォルダ**] をクリックします。(sample_data と同じ階層にフォルダが作られるようにします)\n",
        "*  作成されたフォルダに **files** という名前を設定します。\n",
        "*  files フォルダにカーソルを合わせ、3 点リーダアイコンをクリックして、[**アップロード**] をクリックします。\n",
        "*  ローカルの files フォルダにあるすべてのファイルを選択してアップロードします。ご自身で用意したファイルをアップロードして使用しても構いません。\n",
        "*  「警告」のポップアップが表示されますが問題ありません。[**OK**] をクリックしてポップアップを閉じます。"
      ],
      "metadata": {
        "id": "FH5mLQyujAyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "pclZg_IOh173"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 1: 関数の作成\n",
        "---\n",
        "RAG の各処理を実行する関数を作成します。  \n",
        "タスク (Task) になっている各セルのコードの不足している部分を補完して処理を実装してください。"
      ],
      "metadata": {
        "id": "8vl4ko_WSLfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: ドキュメントをロードする関数の作成\n",
        "Python と LangChain フレームワークを使用して、PDF、Word (.docx)、およびテキスト (.txt) の各ドキュメントフォーマットに応じて動的にドキュメントをロードする関数を作成します。  \n",
        "ファイル拡張子に基づいて適切な Document loader を選択し、対応するライブラリを使用してドキュメントをロードするコードを書いてください。  \n",
        "\n",
        "* ファイル名から拡張子を抽出し、それに基づいて適切な Document loader を使用します\n",
        "* 他の拡張子の場合は、対応していない旨のメッセージを表示して、None を返します\n",
        "* 各ファイル形式に対応する Document loader を動的にインポートするコードを記述します。必要なインポートは、条件分岐の中で行います\n",
        "* ロードプロセス中の状況を示すために、処理中のファイル名を `print` で表示します\n",
        "* ドキュメントの内容をロードし、`data` 変数に格納して関数の出力として返します\n",
        "  \n",
        "参考：  \n",
        "[langchain_community.document_loaders.text.TextLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.text.TextLoader.html)  \n",
        "[langchain_community.document_loaders.pdf.PyPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html)  \n",
        "[langchain_community.document_loaders.word_document.Docx2txtLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.word_document.Docx2txtLoader.html)  \n",
        "[langchain_community.document_loaders.wikipedia.WikipediaLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.wikipedia.WikipediaLoader.html)  \n",
        "https://python.langchain.com/docs/integrations/document_loaders/"
      ],
      "metadata": {
        "id": "sGrOFjVYe8gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_document(file):\n",
        "    import os\n",
        "    name, extension = os.path.splitext(file)\n",
        "\n",
        "    if extension == '.pdf':\n",
        "        from langchain_community.document_loaders.pdf import PyPDFLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = PyPDFLoader(file)\n",
        "    elif extension == '.docx':\n",
        "        from langchain_community.document_loaders.word_document import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "    elif extension == '.txt':\n",
        "        from langchain_community.document_loaders.text import TextLoader\n",
        "        loader = TextLoader(file)\n",
        "    else:\n",
        "        print('Document format is not supported!')\n",
        "        return None\n",
        "\n",
        "    data = loader.load()\n",
        "    return data"
      ],
      "metadata": {
        "id": "jSvyGZMvNOMs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#wikipedia\n",
        "\n",
        "def load_from_wikipedia(query, lang=\"en\", load_max_docs=2):\n",
        "  from langchain.document_loaders import WikipediaLoader\n",
        "  loader = WikipediaLoader(query, lang, load_max_docs)\n",
        "  data = loader.load()\n",
        "  return data"
      ],
      "metadata": {
        "id": "nU8boaU31bMb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: ドキュメントをチャンクに分割する関数の作成\n",
        "ロードしたドキュメント データを指定されたサイズにチャンク分割する関数を作成します。  \n",
        "\n",
        "* Text splitter には `RecursiveCharacterTextSplitter` クラスを使用します\n",
        "* `chunk_size` パラメータを使用して、各チャンクのサイズを指定します\n",
        "* `chunk_overlap` パラメータを使用して、隣接するチャンク間の重複サイズを設定します\n",
        "* これらのパラメータは関数の引数として指定でき、デフォルト値はそれぞれ `256` と `0` です\n",
        "* インスタンス化した `RecursiveCharacterTextSplitter` を使用して、入力されたドキュメント データを分割します\n",
        "* 関数は、分割されたチャンクを `chunks` というリストとして返します。\n",
        "  \n",
        "参考：  \n",
        "[langchain_text_splitters.character.RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html)"
      ],
      "metadata": {
        "id": "nsdhIlhCe0YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_data(data, chunk_size=256, chunk_overlap=0):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size, chunk_overlap)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ewvGumJ0enHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Embedding を行い Vector store インスタンスを出力する関数の作成\n",
        "ドキュメントのチャンクの embeddings を Pinecone の Index に挿入するか、既存の Index から embeddings を取得する関数を作成します。Index の存在チェック、インデックスの作成、embeddings の生成および挿入の処理を実装します。\n",
        "\n",
        "* この関数は引数として、Index 名 (`index_name`) とチャンクのリスト (`chunks`) を受け取ります\n",
        "* Vector store には Pinecone を使用します\n",
        "* Embedding model には OpenAI の `text-embedding-3-small` を使用します。embedding のベクトル次元数は `1536` です\n",
        "* Pinecone クライアントの `list_indexes()` メソッドを使用して、指定された `index_name` が既に存在するかどうかを確認します\n",
        "* Index が存在する場合は、既存の Index から Pinecone の Vector store インスタンスを作成します\n",
        "* Index が存在しない場合は、新しい Index を作成します。create_index メソッドを使用して、指定された次元数 (`1536`) とコサイン類似度 (`cosine`) を使用して Index を作成します\n",
        "* チャンクの embeddings を生成し、それらを新しい Index に挿入し、Pinecone の Vector store インスタンスを作成します\n",
        "* 最後に、Vector store インスタンス `vector_store` を関数の出力として返します\n",
        "\n",
        "参考：  \n",
        "[langchain_openai.embeddings.base.OpenAIEmbeddings](https://python.langchain.com/api_reference/openai/embeddings/langchain_openai.embeddings.base.OpenAIEmbeddings.html)  \n",
        "[langchain_pinecone.vectorstores.PineconeVectorStore](https://python.langchain.com/api_reference/pinecone/vectorstores/langchain_pinecone.vectorstores.PineconeVectorStore.html#pineconevectorstore)  \n",
        "https://python.langchain.com/docs/integrations/vectorstores/pinecone/  \n"
      ],
      "metadata": {
        "id": "srBEYDTBflJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_or_fetch_embeddings(index_name, chunks):\n",
        "    # 必要なライブラリをインポート\n",
        "    import pinecone\n",
        "    from       import PineconeVectorStore\n",
        "    from       import OpenAIEmbeddings\n",
        "    from pinecone import PodSpec, ServerlessSpec\n",
        "\n",
        "    # Pinecone クライアントを初期化\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    # Embedding model のインスタンスを作成\n",
        "    embedding_model =\n",
        "\n",
        "    # embeddings の作成/ロード、Vector store の作成\n",
        "    if index_name in pc.list_indexes().names():\n",
        "        # Index がすでに存在する場合\n",
        "        print(f'Index {index_name} already exists. Loading embeddings ... ')\n",
        "        # Vector store インスタンスを作成\n",
        "        vector_store =\n",
        "        print('Ok')\n",
        "    else:\n",
        "        # Index が存在しない場合\n",
        "        print(f'Creating index {index_name} and embeddings ... ')\n",
        "\n",
        "        # Index を作成\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=ServerlessSpec(\n",
        "                cloud='aws',\n",
        "                region='us-east-1'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Vector store インスタンスを作成\n",
        "        vector_store =\n",
        "        print('Ok')\n",
        "\n",
        "    return vector_store"
      ],
      "metadata": {
        "id": "szv_vHrofgNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Index を削除する関数"
      ],
      "metadata": {
        "id": "HcYOmeAbgh8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')"
      ],
      "metadata": {
        "id": "bbZzJ4fyggzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: RAG Chain を実行する関数\n",
        "---\n",
        "会話履歴を考慮してユーザーの質問に回答する Chain を実行する関数を作成します。  \n",
        "このタスクでは、Retriever の作成、会話履歴を考慮した質問の再構成、そして質問に対する適切な応答を生成する Chain を作成します。  \n",
        "\n",
        "* この関数は引数として、`vector_store` 、ユーザーの質問 `q` 、Chat history のインスタンス `chat_history` 、セッション ID `session_id` (デフォルト値は `unused`) 、検索結果ドキュメントの取得数 `k` (デフォルト値は `20`) を受け取ります\n",
        "* Chat model には OpenAI の `gpt-4o-mini` を使用します\n",
        "* Retriever にも会話履歴を考慮させます\n",
        "* LLM からの回答はテキストの形式に変換します\n",
        "* `RunnableWithMessageHistory` を使用して Chain の処理に会話履歴を組み込みます\n",
        "* Chain 処理の出力 `answer` を関数の出力として返します\n",
        "\n",
        "参考：  \n",
        "https://python.langchain.com/docs/concepts/#retrievers  \n"
      ],
      "metadata": {
        "id": "sD9Xzc8_1mQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer_with_history(vector_store, q, chat_history, session_id='unused', k=20):\n",
        "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "    from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "    from langchain_openai import ChatOpenAI\n",
        "    from operator import itemgetter\n",
        "    from langchain_core.output_parsers import StrOutputParser\n",
        "    from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "    from langchain_core.chat_history import BaseChatMessageHistory\n",
        "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "    from langchain.chains import create_history_aware_retriever\n",
        "\n",
        "    # Chat model\n",
        "    model =\n",
        "\n",
        "    # Retriever\n",
        "    # パラメータは search_type='similarity', search_kwargs={'k': k} としてください\n",
        "    retriever =\n",
        "\n",
        "    # 検索用にクエリを書き換えるためのプロンプト\n",
        "    contextualize_q_system_prompt = (\n",
        "        \"Given a chat history and the latest user question \"\n",
        "        \"which might reference context in the chat history, \"\n",
        "        \"formulate a standalone question which can be understood \"\n",
        "        \"without the chat history. Do NOT answer the question, \"\n",
        "        \"just reformulate it if needed and otherwise return it as is.\"\n",
        "    )\n",
        "\n",
        "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", contextualize_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", \"{input}\"),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 会話履歴を考慮する Retriever を作成\n",
        "    history_aware_retriever =\n",
        "\n",
        "\n",
        "    # ユーザーのクエリに回答させるための Prompt template\n",
        "    system_message = \"\"\"以下の参考用のテキストの一部を参照して、質問に回答してください。もし参考用のテキストの中に回答に役立つ情報が含まれていなければ、分からない、と答えてください。\n",
        "\n",
        "    {context}\"\"\"\n",
        "    human_message = \"質問：{input}\"\n",
        "\n",
        "    chat_prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "        (\n",
        "            \"system\", system_message\n",
        "        ),\n",
        "        ,\n",
        "        (\n",
        "            \"human\", human_message\n",
        "        )\n",
        "    ])\n",
        "\n",
        "    # ユーザーのクエリと history_aware_retriever が取得した Documents を出力する Runnable\n",
        "    add_context = RunnablePassthrough.assign(context=history_aware_retriever)\n",
        "\n",
        "    # Chain を定義\n",
        "    rag_chain =\n",
        "\n",
        "    runnable_with_history = RunnableWithMessageHistory(\n",
        "\n",
        "        lambda session_id: chat_history, # session_id を受け取って対応する chat message history インスタンス (BaseChatMessageHistory) を返す関数\n",
        "\n",
        "\n",
        "    )\n",
        "\n",
        "    # Chain の実行\n",
        "    answer =\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Pikqr8mMQa4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 2: 関数を使用した処理の実行\n",
        "---\n",
        "Section 1 で作成した関数を使用して RAG の実行処理を実装します。\n",
        "タスク (Task) になっている各セルのコードの不足している部分を補完して処理を実装してください。"
      ],
      "metadata": {
        "id": "8ti6JVLkQc9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: ドキュメントのロードとチャンク化\n",
        "* ファイルからドキュメントのデータをロードし、それをチャンクに分割します\n",
        "* チャンクのサイズは `300` 、チャンク間の重複サイズは `0` とします"
      ],
      "metadata": {
        "id": "BiPtImWzfOiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ドキュメント データをロード\n",
        "data =\n",
        "\n",
        "# チャンクに分割\n",
        "chunks =\n",
        "\n",
        "# 確認のため、チャンク数を表示する\n",
        "print(len(chunks))"
      ],
      "metadata": {
        "id": "6keUaWOlw2pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 既存の Index を削除"
      ],
      "metadata": {
        "id": "XXpjVpOgKLXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delete_pinecone_index()"
      ],
      "metadata": {
        "id": "8cjQkI95xKM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Vector store インスタンスを取得\n",
        "Index 名を指定して、その Vector store インスタンスを取得します"
      ],
      "metadata": {
        "id": "hzdJPfTXKRoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index 名を指定\n",
        "index_name = 'askadocument'\n",
        "\n",
        "# Vector store インスタンスを取得\n",
        "vector_store ="
      ],
      "metadata": {
        "id": "Vmkn0nBtxNj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: チャットボットを実行\n",
        "* Chat history には `ChatMessageHistory` を使用します\n",
        "* 必要な引数を `get_answer_with_history` 関数に渡して実行し、回答を表示します\n",
        "* `get_answer_with_history` 関数に引数として渡す session_id は任意の英数字列で構いません"
      ],
      "metadata": {
        "id": "pEoYp80zKYjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.chat_message_histories.in_memory import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "\n",
        "# Chat history\n",
        "chat_history =\n",
        "\n",
        "import time\n",
        "i = 1\n",
        "print('Write Quit or Exit to quit.')\n",
        "while True:\n",
        "    q = input(f'Question #{i}: ')\n",
        "    i = i + 1\n",
        "    if q.lower() in ['quit', 'exit']:\n",
        "        print('Quitting ... bye bye!')\n",
        "        time.sleep(2)\n",
        "        break\n",
        "\n",
        "    answer =\n",
        "    print(f'\\nAnswer: {answer}')\n",
        "    print(f'\\n {\"-\" * 50} \\n')"
      ],
      "metadata": {
        "id": "pLm4DgGXxlgv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}